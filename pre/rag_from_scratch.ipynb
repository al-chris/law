{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\FIDELIS ALIU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. This approach, often used in conjunction with Chain of Thought prompting, helps AI models understand and execute complex tasks by addressing them step-by-step.  This can be achieved through simple prompting techniques, task-specific instructions, or human input. \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain  import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=GoogleGenerativeAIEmbeddings(model= \"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API_KEY\")))\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"what is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favourite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embd = GoogleGenerativeAIEmbeddings(model= \"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "query_result = embd.embed_query(question)\n",
    "documnt_result = embd.embed_query(document)\n",
    "len(query_result), len(documnt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine similarity (1 indicates identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8456968223547316\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, documnt_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEXING ####\n",
    "\n",
    "# load blog\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitter\n",
    "\n",
    "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chuks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest related semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=GoogleGenerativeAIEmbeddings(model= \"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=GoogleGenerativeAIEmbeddings(model= \"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# k is the number of nearby neighbors you want to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FIDELIS ALIU\\AppData\\Local\\Temp\\ipykernel_4512\\4059233835.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt \n",
    "template = \"\"\"Answer the question based only on the following context\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt =  ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition is a technique used by LLM-powered autonomous agents to break down complex tasks into smaller, more manageable steps. This can be achieved through various methods:\\n\\n1. **Chain of thought (CoT) prompting:** Instructing the model to \"think step by step,\" allowing it to decompose the task into simpler subtasks.\\n2. **Tree of Thoughts (ToT):** Expanding on CoT by exploring multiple reasoning possibilities at each step, creating a tree-like structure of potential solutions.\\n3. **LLM prompting:** Using prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the model in breaking down the task.\\n4. **Task-specific instructions:** Providing instructions tailored to the specific task, such as \"Write a story outline\" for novel writing.\\n5. **Human input:** Directly providing the task decomposition steps to the model. \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-988ef316-eff7-4935-9b53-769c138dbeb7-0', usage_metadata={'input_tokens': 341, 'output_tokens': 188, 'total_tokens': 529})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": \"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used by LLM-powered autonomous agents to break down complex tasks into smaller, more manageable steps. This is often achieved through prompting techniques like Chain of Thought (CoT) where the model is instructed to \"think step by step\". \\n\\nHere\\'s how task decomposition can be done:\\n\\n1. **Simple Prompting:** Asking the LLM to list steps or subgoals for a task (e.g., \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\")\\n2. **Task-Specific Instructions:** Providing instructions tailored to the task, like \"Write a story outline\" for novel writing.\\n3. **Human Input:** Directly involving humans in breaking down the task. \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,\n",
    "     \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"what is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXXING ####\n",
    "\n",
    "\n",
    "# load blog\n",
    "import bs4\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docsdocs = loader.load()\n",
    "\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "\n",
    "# Index\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=GoogleGenerativeAIEmbeddings(model= \"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help \n",
    "the user overcome some of the limitaions of the distance-based similarity search. \n",
    "Provide the alternative questions separated by newlines. Original Question: {question}\"\"\"\n",
    "\n",
    "prompt_prespectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_prespectives\n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial process for LLM-powered agents to handle complex tasks. It involves breaking down a large, intricate task into a sequence of smaller, more manageable subtasks. This approach makes it easier for the LLM to understand and address the problem systematically. \\n\\nHere\\'s a breakdown of task decomposition in LLM agents:\\n\\n**Methods of Task Decomposition:**\\n\\n* **Chain-of-Thought (CoT) Prompting:** This technique encourages the LLM to \"think step-by-step\" by explicitly prompting it to outline the steps required to complete the task. This helps in decomposing the problem into smaller, more interpretable steps.\\n* **Tree of Thoughts (ToT):**  An extension of CoT, ToT explores multiple reasoning possibilities at each step, creating a tree-like structure of potential solutions. This allows for a broader exploration of the solution space and can lead to more robust solutions.\\n* **LLM-based Decomposition:** LLMs can be directly prompted to decompose tasks using simple instructions like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\".\\n* **Task-Specific Instructions:**  Providing the LLM with instructions tailored to the specific task, such as \"Write a story outline\" for novel writing, can guide the decomposition process effectively.\\n* **Human Input:** In some cases, human input can be used to decompose the task into subtasks, especially when domain expertise is required.\\n\\n**Benefits of Task Decomposition:**\\n\\n* **Improved Performance:**  Breaking down complex tasks makes them easier for the LLM to process, leading to better overall performance.\\n* **Enhanced Interpretability:**  The step-by-step breakdown provides insights into the LLM\\'s reasoning process, making it easier to understand how it arrived at a solution.\\n* **Facilitates Planning:**  Task decomposition is a crucial aspect of planning for LLM agents, allowing them to strategize and prioritize subtasks effectively.\\n\\n**Challenges:**\\n\\n* **Long-Term Planning:**  LLMs still struggle with planning over extended time horizons and adapting to unexpected errors during task execution.\\n* **Effective Exploration:**  Exploring the vast solution space efficiently, especially in ToT, remains a challenge.\\n\\nOverall, task decomposition is a fundamental process for LLM agents to tackle complex tasks effectively. As research progresses, we can expect to see more sophisticated and efficient methods for task decomposition, leading to more capable and reliable LLM-powered agents. \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAF-Fusion; Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal Rank Fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used in the RRF formula. \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate over each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its  rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the documet to a string format to use as  a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with a score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any \n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1/(rank * k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "        \n",
    "    # Sort the documents by their fused scores in descending order to get the final ranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents is the process of breaking down a complex task into smaller, more manageable subtasks. This allows the agent to tackle the problem more efficiently and effectively. \\n\\nHere are some key methods of task decomposition mentioned in the context:\\n\\n* **Chain of Thought (CoT):** This technique prompts the LLM to \"think step by step,\" decomposing the task into a sequence of smaller steps.\\n* **Tree of Thoughts:** This method expands on CoT by exploring multiple reasoning possibilities at each step, creating a tree-like structure of potential solutions.\\n* **Prompt Engineering:**  Simple prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" can guide the LLM to decompose the task.\\n* **Task-Specific Instructions:**  Providing instructions tailored to the specific task, such as \"Write a story outline\" for writing a novel, can aid in decomposition.\\n* **Human Input:**  Humans can directly assist in breaking down the task into subtasks. \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_rag_fusion,\n",
    "        \"question\": RunnablePassthrough() # I'm testing to see if it'll work instead of itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Only return the sub-questions \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run \n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the core functions of an autonomous agent? ',\n",
       " '2. How do Large Language Models (LLMs) contribute to autonomous agent capabilities?',\n",
       " '3. What are the typical architectural components found in an LLM-powered autonomous agent system? ',\n",
       " '']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question - answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question - answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\" Format Q and A pairs \"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}  \\nAnswer: {answer} \\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "\n",
    "#\n",
    "q_a_pairs =\"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")\n",
    "        }\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided text snippets offer a glimpse into the capabilities of autonomous agents, particularly those powered by Large Language Models (LLMs), but they don\\'t directly answer a specific question. \\n\\nHowever, we can infer that the snippets relate to the concept of **goal-oriented autonomous agents** and their potential applications. \\n\\nHere\\'s a breakdown:\\n\\n1. **Goal-Driven Behavior:** The repeated mention of \"GOALS\" followed by placeholders for \"user-provided goal\" strongly suggests a system designed to pursue and achieve specific objectives defined by the user.\\n\\n2. **LLM as the Core:** While not explicitly stated, the agent\\'s ability to \"inquire,\" \"select,\" \"request,\" and \"attempt synthesis\" implies a level of language understanding, reasoning, and decision-making that aligns with the capabilities of LLMs.\\n\\n3. **Scientific Discovery Example:** The example of an agent working in drug discovery (\"inquired about current trends in anticancer drug discovery,\" \"selected a target,\" \"requested a scaffold,\" \"attempted its synthesis\") illustrates a potential real-world application of such goal-oriented agents in complex domains.\\n\\n4. **Constraints and Memory:** The listed \"Constraints\" provide insights into the agent\\'s operational limitations, such as short-term memory limits and reliance on specific commands. This highlights the challenges of designing agents that can operate effectively within real-world constraints.\\n\\n5. **AutoGPT Connection:** The format of the \"GOALS\" section and the mention of \"short-term memory\" closely resemble the system messages used in AutoGPT, suggesting that the snippets might be describing a similar or related autonomous agent framework.\\n\\n**In essence, the provided text fragments hint at a powerful concept: autonomous agents that can understand user-defined goals, leverage LLMs for reasoning and decision-making, and potentially revolutionize fields like scientific discovery by automating complex tasks.** \\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAnswer each sub-question individually\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\" RAG on each sub-question \"\"\"\n",
    "\n",
    "    # Use our decomposition\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "\n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\n",
    "            \"context\": retrieved_docs,\n",
    "            \"question\": sub_question\n",
    "        })\n",
    "\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An LLM-powered autonomous agent system can be broken down into several key components:\\n\\n**1. The Large Language Model (LLM):** This acts as the **brain** of the system. It provides the intelligence to understand instructions, break down complex tasks into smaller sub-goals, and generate plans to achieve those goals. The LLM's ability to learn from its actions and improve over time is crucial for the agent's autonomy.\\n\\n**2. Goal Management System:** This component defines the agent's objectives and constraints. It could be a simple set of instructions or a more complex system that dynamically adjusts goals based on the environment and feedback. \\n\\n**3. Action Execution Engine:** This component translates the LLM's plans into concrete actions within a specific environment. For example, it might interact with APIs, databases, or even physical systems depending on the agent's purpose.\\n\\n**4. Feedback and Learning Mechanism:** This component allows the agent to learn from its experiences. It analyzes the outcomes of its actions against the intended goals and feeds this information back into the LLM, enabling it to refine its strategies and improve future performance.\\n\\n**5. Ethical and Safety Considerations:** While not a separate component, ethical and safety considerations are paramount in designing and deploying autonomous agents. These considerations should be integrated into all aspects of the system, from goal setting and action selection to feedback mechanisms, ensuring responsible and beneficial use.\\n\\n**AutoGPT**, as mentioned in your provided context, exemplifies these components. It utilizes an LLM to manage goals, generate plans, and execute actions, showcasing the potential of LLM-powered autonomous agents. However, it's important to note that this field is still evolving, and further research is needed to address challenges related to reliability and safety. \\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\" Format Q and A pairs \"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}  \\nAnswer {i}: {answer} \\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = ( prompt | llm | StrOutputParser() )\n",
    "\n",
    "final_rag_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel's personal history?\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# We now transform these to example messages\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\"\n",
    "        ),\n",
    "        # Few-shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is task decomposition? \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial component in building LLM-powered autonomous agents, enabling them to tackle complex tasks effectively. It involves breaking down a large, intricate task into a sequence of smaller, more manageable subtasks. This approach offers several benefits:\\n\\n* **Improved Performance:** LLMs often struggle with complex instructions. Decomposing tasks into smaller steps allows the model to focus on one aspect at a time, leading to better overall performance.\\n* **Enhanced Reasoning:** By explicitly outlining the steps involved, task decomposition encourages the LLM to engage in a more structured and logical reasoning process, similar to how humans solve problems.\\n* **Interpretability:**  Breaking down the task makes the LLM\\'s thought process more transparent, allowing developers to understand how the model arrived at a particular solution and identify potential areas for improvement.\\n\\nHere are some key methods for task decomposition in LLM agents:\\n\\n**1. Chain of Thought (CoT) Prompting:**\\n\\n* This technique involves prompting the LLM to \"think step-by-step\" when solving a problem. \\n* By explicitly asking the model to articulate its reasoning process, CoT encourages the decomposition of the task into smaller, logical steps.\\n* For example, instead of directly asking \"What is the capital of France?\", you would prompt the model with \"The capital of France is a well-known city in Europe. France is known for its romantic atmosphere and iconic landmarks. What is the capital of France?\". This encourages the model to reason through the clues and arrive at the answer: \"Paris\".\\n\\n**2. Tree of Thoughts (ToT):**\\n\\n* ToT extends the CoT approach by exploring multiple reasoning possibilities at each step, creating a tree-like structure of potential solutions.\\n* This method allows the agent to consider different approaches and select the most promising path, leading to more robust and reliable problem-solving.\\n* For example, when planning a trip, ToT would explore various options for each step (transportation, accommodation, activities) and evaluate their feasibility before settling on the optimal plan.\\n\\n**3.  LLM-Based Decomposition:**\\n\\n* LLMs themselves can be used to decompose tasks by providing them with simple prompts.\\n* For instance, asking the LLM \"Steps for writing an essay.\\\\n1.\" or \"What are the subgoals for achieving [task]?\" can elicit a structured breakdown of the task.\\n\\n**4. Task-Specific Instructions:**\\n\\n* In some cases, task decomposition can be achieved through specific instructions tailored to the task at hand.\\n* For example, when writing a novel, the instruction \"Write a story outline\" naturally guides the LLM to break down the writing process into defining characters, plot points, and structuring the narrative.\\n\\n**5. Human Input:**\\n\\n* While the goal is to automate task decomposition, human input can be valuable, especially in complex or ambiguous scenarios.\\n* Humans can provide initial guidance on how to break down the task or refine the LLM-generated decomposition to ensure accuracy and efficiency.\\n\\nIn conclusion, task decomposition is an essential technique for building effective LLM-powered agents. By breaking down complex tasks into manageable steps, we can enhance the reasoning abilities, performance, and interpretability of these agents, paving the way for more sophisticated and capable AI systems. \\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt\n",
    "response_prompt_template = \"\"\"You are an expert at world knowledge. I am going to ask you a question. Your response should be conprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer: \"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Task Decomposition for Large Language Model Agents: A Bridge to Complex Action\\n\\nLarge language models (LLMs) have demonstrated remarkable abilities in understanding and generating human-like text. However, their application as agents capable of interacting with complex environments and achieving multi-step goals requires a mechanism to translate high-level instructions into actionable steps. This mechanism is known as **task decomposition**. \\n\\nTask decomposition for LLM agents involves breaking down a complex task, often expressed in natural language, into a sequence of smaller, more manageable subtasks. These subtasks are designed to be atomic, meaning they can be executed by the agent directly or through simple API calls. This decomposition process effectively bridges the gap between the LLM's high-level understanding of the task and the granular actions required for its execution.\\n\\nSeveral approaches to task decomposition exist, ranging from predefined hierarchical structures to dynamic planning methods leveraging the LLM's reasoning capabilities. These methods may utilize:\\n\\n* **Predefined skills:**  A library of pre-programmed functions representing basic actions the agent can perform. The decomposition process involves mapping subtasks to these existing skills.\\n* **Dynamic planning:** The LLM itself reasons about the task and generates a sequence of subtasks, potentially utilizing external knowledge sources or tools.\\n* **Hybrid approaches:** Combining predefined skills with dynamic planning, allowing for both efficiency and flexibility in task execution.\\n\\nEffective task decomposition is crucial for LLM agents to operate successfully in real-world scenarios. It enables them to tackle complex goals by breaking them down into manageable steps, ultimately expanding their capabilities and potential applications. \\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document operation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"), temperature=0)\n",
    "\n",
    "generate_docs_for_retrieval = prompt_hyde | model | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents is the process of breaking down a complex task into smaller, more manageable subtasks. This allows the agent to tackle the problem more efficiently and effectively. \\n\\nHere\\'s how it works:\\n\\n* **Chain of Thought (CoT):** The LLM is prompted to \"think step-by-step,\" decomposing the task into a sequence of smaller steps. This helps the model understand and solve complex problems by breaking them down.\\n* **Tree of Thoughts:** This method expands on CoT by exploring multiple reasoning possibilities at each step. It generates a tree-like structure of potential solutions, allowing for a more comprehensive exploration of the problem space.\\n* **Methods of Decomposition:** Task decomposition can be achieved through:\\n    * **Simple Prompting:** Using prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\"\\n    * **Task-Specific Instructions:** Providing instructions tailored to the specific task, such as \"Write a story outline\" for writing a novel.\\n    * **Human Input:**  Involving humans in the process of breaking down the task.\\n\\nBy decomposing tasks, LLM agents can better understand complex problems, explore various solution paths, and ultimately achieve better performance in completing the assigned tasks. \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = prompt | model | StrOutputParser()\n",
    "\n",
    "final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
